
![logo](https://github.com/user-attachments/assets/71c84b05-1876-4c5b-8e66-f8c59915ada4)


# Welcome to LLM Compass

## The ultimate collection of resources for building, evaluating, and mastering Large Language Models.
---

## üìö Libraries & Frameworks
- [Haystack](https://github.com/deepset-ai/haystack) ‚Äì Production-ready framework for building search engines, RAG systems, and question-answering applications.
- [Hugging Face Transformers](https://github.com/huggingface/transformers) ‚Äì Hugely popular NLP library providing thousands of pre-trained models for text generation, classification, translation, and fine-tuning.
- [LangChain](https://github.com/langchain-ai/langchain) ‚Äì Flexible framework for building real-world LLM-powered applications such as RAG, agents, and pipelines.
- [LLaMA](https://github.com/facebookresearch/llama) ‚Äì Meta‚Äôs family of open-source LLMs that provide strong performance for research and downstream tasks.
- [llama.cpp](https://github.com/ggerganov/llama.cpp) ‚Äì Highly efficient inference engine for LLaMA models on CPU, optimized for local deployment.
- [OpenAI GPT API](https://platform.openai.com/docs/api-reference) ‚Äì Official API for integrating GPT models into apps, chatbots, and workflows with robust support.

---

## üß™ Evaluation & Testing Tools
- [FastChat](https://github.com/lm-sys/FastChat/tree/main) ‚Äì An open platform for training, serving, and evaluating large language model based chatbots.
- [Helm](https://crfm.stanford.edu/helm/latest/) ‚Äì Stanford‚Äôs holistic evaluation suite for analyzing accuracy, robustness, calibration, and fairness of LLMs.
- [llm-testlab](https://github.com/Saivineeth147/llm-testlab) ‚Äì Comprehensive toolkit for evaluating LLM responses on hallucinations, consistency, safety, and semantic similarity.
- [OpenAI Evals](https://github.com/openai/evals) ‚Äì Framework for creating, sharing, and running benchmarks to track LLM performance across tasks.

---

## üìä Datasets
- [Dolly 15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) ‚Äì High-quality open dataset of instruction-following examples by Databricks.
- [HelpSteer](https://huggingface.co/datasets/nvidia/HelpSteer) ‚Äì Human preference dataset for guiding LLMs toward helpful, safe, and ethical outputs.
- [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) ‚Äì Open-source reproduction of the WebText dataset used to train GPT models.
- [Pile](https://pile.eleuther.ai/) ‚Äì Massive 825GB dataset covering diverse domains for training robust large-scale models.
- [RedPajama](https://github.com/togethercomputer/RedPajama-Data) ‚Äì Large-scale dataset replicating the training data for state-of-the-art LLMs.
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) ‚Äì Instruction-following dataset built on LLaMA for research in alignment and fine-tuning.

---

## üéì Tutorials & Guides
- [FreeCodeCamp Guide](https://www.freecodecamp.org/news/a-beginners-guide-to-large-language-models/) ‚Äì Beginner‚Äôs guide to LLMs with practical examples and simple explanations.
- [Google: Intro to LLMs](https://developers.google.com/machine-learning/resources/intro-llms) ‚Äì Accessible guide to understanding LLMs, transformers, and training basics.
- [Hugging Face LLM Course](https://huggingface.co/learn/llm-course/chapter1/1) ‚Äì Practical, hands-on course to learn transformers, fine-tuning, and deployment.
- [LangChain Tutorials](https://python.langchain.com/docs/tutorials/) ‚Äì Official tutorials on building advanced LLM pipelines and AI applications.
- [Microsoft Generative AI for Beginners](https://learn.microsoft.com/en-us/shows/generative-ai-for-beginners/) ‚Äì Beginner-friendly video series explaining generative AI concepts and use cases.
- [mlabonne/llm-course](https://github.com/mlabonne/llm-course) ‚Äì Open-source curriculum teaching LLM theory, fine-tuning, and applications.
- [OpenAI Cookbook](https://github.com/openai/openai-cookbook) ‚Äì Collection of examples, patterns, and recipes for leveraging GPT effectively.
- [Stanford Lecture: Intro to LLMs](https://www.youtube.com/watch?v=zjkBMFhNj_g) ‚Äì Detailed lecture explaining the architecture, training, and applications of LLMs.

---

## üìÑ Research Papers
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) ‚Äì Seminal paper introducing the Transformer architecture that underpins modern LLMs.
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165) ‚Äì Landmark paper on GPT-3 demonstrating few-shot learning capabilities.
- [LLM Evaluation Surveys](https://arxiv.org/abs/2307.03109) ‚Äì Comprehensive survey of evaluation strategies for large language models.
- [RLHF: Training Language Models to Follow Instructions](https://arxiv.org/abs/2203.02155) ‚Äì Research introducing Reinforcement Learning with Human Feedback for alignment.
- [Stanford Alpaca Paper](https://arxiv.org/abs/2303.16199) ‚Äì Study on fine-tuning LLaMA with lightweight instruction datasets.

---

## üöÄ Example Projects
- [Auto-GPT](https://github.com/Torantulino/Auto-GPT) ‚Äì Autonomous GPT-4 agent capable of planning and executing multi-step tasks automatically.
- [BabyAGI](https://github.com/yoheinakajima/babyagi) ‚Äì Lightweight autonomous agent using LLMs for iterative goal-setting and task execution.
- [ChatGPT-Next-Web](https://github.com/Yidadaa/ChatGPT-Next-Web) ‚Äì Self-hosted ChatGPT-like web app with customizable UI and backend.
- [GPT Engineer](https://github.com/AntonOsika/gpt-engineer) ‚Äì Tool for generating complete codebases from natural language project descriptions.
- [PrivateGPT](https://github.com/imartinez/privateGPT) ‚Äì Privacy-focused tool for chatting with documents locally without internet or cloud access.

---

## üåç Communities
- [Discord: AI Exchange](https://discord.gg/aiexchange) ‚Äì Discussion hub for generative AI trends, tools, and project showcases.
- [Discord: EleutherAI](https://discord.gg/eleutherai) ‚Äì Research collective collaborating on open LLMs, datasets, and reproducibility.
- [Discord: Hugging Face](https://huggingface.co/join/discord) ‚Äì Official Hugging Face server with channels for models, datasets, and developer support.
- [Discord: LangChain](https://discord.gg/langchain) ‚Äì Active server for developers working with LangChain to share projects and solve issues.
- [Discord: OpenAccess AI Collective](https://discord.gg/openaccess-ai) ‚Äì Group focused on democratizing AI and sharing open-source resources.
- [Reddit: r/ArtificialInteligence](https://www.reddit.com/r/ArtificialInteligence/) ‚Äì Active subreddit covering AI news, breakthroughs, and applications.
- [Reddit: r/ChatGPT](https://www.reddit.com/r/ChatGPT/) ‚Äì Dedicated community discussing ChatGPT use cases, tips, and creative experiments.
- [Reddit: r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/) ‚Äì Focused community for running LLaMA and open-source models locally on personal hardware.
- [Reddit: r/MachineLearning](https://www.reddit.com/r/MachineLearning/) ‚Äì One of the largest ML/AI research communities with discussions on models, papers, and breakthroughs.

---

## üèÜ Top LLMs & Benchmarks (2025)

- [Claude Opus 4 (Anthropic)](https://www.anthropic.com/news/claude-4) ‚Äì Strengths: Advanced reasoning, coding, and multimodal capabilities | Benchmarks: GPQA Science 79.6%, LiveCodeBench 72%, USAMO 21.7%, HMMT 58.3%, AIME 75.5%, ARC-AGI-2 8.6% | Notes: Anthropic's most capable model yet, setting new standards in reasoning, coding, and complex math.
- [Claude Sonnet 4 (Anthropic)](https://www.anthropic.com/news/claude-4) ‚Äì Strengths: Efficient performance for everyday tasks | Benchmarks: GPQA Science 79.6%, LiveCodeBench 72%, USAMO 21.7%, HMMT 58.3%, AIME 75.5%, ARC-AGI-2 8.6% | Notes: Smart, efficient model for everyday use.
- [DeepSeek-V3.1](https://api-docs.deepseek.com/news/news250821?utm_source=chatgpt.com) ‚Äì Strengths: Coding and reasoning-focused tasks | Benchmarks: MMLU-Redux 91.8%, SWE-Bench 66% | Notes: Optimized for hybrid thinking and agentic workflows, strong in coding challenges.
- [Grok 4 (xAI)](https://x.ai/news/grok-4) ‚Äì Strengths: General reasoning and structured output | Benchmarks: GPQA Science 86.4%, LiveCodeBench 79%, USAMO 37.5%, HMMT 90%, AIME 91.7%, ARC-AGI-2 15.9% | Notes: Balanced model for math, reasoning, and coding.
- [Grok 4 Heavy w/ Python (xAI)](https://x.ai/news/grok-4) ‚Äì Strengths: Top coding, reasoning, and math performance | Benchmarks: GPQA Science 88.4%, LiveCodeBench 79.4%, USAMO 61.9%, HMMT 96.7%, AIME 100%, ARC-AGI-2 15.9% | Notes: Best-in-class Grok 4 variant optimized for Python-heavy tasks.
- [Grok 4 w/ Python (xAI)](https://x.ai/news/grok-4) ‚Äì Strengths: Strong coding and reasoning with Python | Benchmarks: GPQA Science 87.5%, LiveCodeBench 79.3%, USAMO 37.5%, HMMT 93.9%, AIME 98.8%, ARC-AGI-2 8.6% | Notes: Efficient for programming-intensive tasks.
- [GPT-5 (OpenAI)](https://platform.openai.com/docs/guides/latest-model?utm_source=chatgpt.com) ‚Äì Strengths: Exceptional reasoning, coding, and multimodal capabilities | Benchmarks: MMLU 91.2%, GPQA 79.3%, SWE-Bench 54.6% | Notes: OpenAI's latest flagship model with a large context window and advanced agentic capabilities.
- [Gemini 2.5 Pro (Google DeepMind)](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro?utm_source=chatgpt.com) ‚Äì Strengths: Multimodal reasoning, translation, and math | Benchmarks: GPQA Science 83.3%, LiveCodeBench 74.2%, USAMO 34.5%, HMMT 82.5%, AIME 88.9%, ARC-AGI-2 4.9% | Notes: Excels at complex interactive and reasoning tasks.
- [Llama 4 (Meta)](https://www.llama.com/models/llama-4/) ‚Äì Strengths: Cost-efficient, local deployment, flexible fine-tuning | Benchmarks: MMLU 85%, GPQA 80%, SWE-Bench 69.4% | Notes: Open-source LLM ideal for research, local inference, and instruction-following.
- [o3 (Open LLM)](https://docs.o3.ai/) ‚Äì Strengths: Reasoning & math tasks | Benchmarks: GPQA Science 79.6%, LiveCodeBench 72%, USAMO 21.7%, HMMT 58.3%, AIME 88.9%, ARC-AGI-2 6.5% | Notes: Competitive math and reasoning model.
- [Qwen 3 (Alibaba)](https://qwen.readthedocs.io/en/latest/?utm_source=chatgpt.com) ‚Äì Strengths: Coding, reasoning, and multilingual support | Benchmarks: SWE-Bench High, AIME 2025 93.3% | Notes: Designed for both language and multimodal tasks with strong domain versatility.



---

## ü§ù Contributing
Contributions welcome! If you find a valuable LLM resource or have an Open Source Project, open a PR.

---

## üìú License
[MIT License](./LICENSE)

